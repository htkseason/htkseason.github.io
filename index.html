<html>
<style>
    a{
      text-decoration: none;
    }
    a:link, a:visited{
      color:#3962F6;
    } 

    a.title:link, a.title:visited{
      color:#173bbd;
      font-size: 18px;
    }

    a.author:link, a.author:visited{
      color:#666666;
    }
    


  body {
    background-color: #ddd;
  }

  h1 {
    font-size: 30px
  }

  img {
    width: 100%;
    height: auto;
  }

  .main {
    width: 800px;
    margin: auto auto;
    background-color: #eee;
    padding: 80px;
    padding-top: 30px;
    font-family: "Times";
  }


  img.me {
    height: 250px;
    width: auto;
    float: right;
    margin-top: -50px;
    margin-left: 30px;
    margin-right: 20px;
  }

  .intro {
    padding-bottom: 10px;
    font-size: 17px;
  }

  .intro-des {
    padding-left: 10px;
  }

  .proj {}

  .proj-table {
    padding-top: 0px;
  }
  td {
    padding: 15px;
    padding-top: 30px;
  }

  .proj-teaser {
    width: 200px;
    height: 200px;
    padding-left: 10px;
    padding-right: 20px;
    padding-bottom: 20px;
  }

  .proj-des {
    line-height: 150%;
    padding-bottom: 20px;
    font-size: 16px;
  }

  .proj-des-highlight {
    color: #FF33CC;
  }

  .proj-detail {
    line-height: 1;
    font-size: 12px;
    color: #3d8a9c;
  }
</style>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="google-site-verification" content="uni4W2fCenO6mAjXfGGTukt5Ua9ydl8pCNS7KUsYcac" />
  <title>
    Zhixiang Min
  </title>
</head>

<body>
  <div class="main">
    <div class="intro">
      <h1>Zhixiang Min</h1>
      <img class="me" src="./me2.jpg">
      <div class="intro-des">
        
        <p>
        I received my Ph.D. (2018-2023) in Computer Science at Stevens Institute of Technology, advised by Prof. <a href="https://www.cs.stevens.edu/~edunn/">Enrique Dunn</a>. 
        I spent summer internships at <a href="https://www.zillow.com/">Zillow</a> and <a href="https://www.nec-labs.com/">NEC Labs</a>. 
        </p>

        <p>
        My research interest lies in 3D computer vision (SfM, SLAM, localization, reconstruction) and recently image-based rendering. 
        My work during PhD focuses on closing the gap between deep learning methods and 3D vision problems. 
        Specifically, I design localization algorithms that utilize deep learning to improve challenging instances and solve ill-posed problems,
        while, importantly, maintaining the geometric interpretabilities. 
        </p>
        
        <p>
          <b>After graduation, I joined Apple VIO/SLAM team, working on AR/VR and Vision Pro. </b>
        </p>

        <p>
          Email: <a href="mailto:zmin1@stevens.edu">zmin1 [at] stevens.edu</a> <br>
          <a href="https://scholar.google.com/citations?user=UR8AjXIAAAAJ">[Google Scholar]</a> / <a href="https://www.linkedin.com/in/zhixiang-min-12b87a1a3/">[LinkedIn]</a> / <a href="./CV - Zhixiang Min - 2023.pdf">[CV]</a>
        </p>

        </p>
      </div>
    </div>

    <div class="proj">
      <h1>Projects</h1>
      (I occasionally fix some typos in our camready submissions. Please refer to the arxiv version for best experience.)
      <div class="proj-table">
        <table>
          <tbody>
          
          <!-- iccv 2023 (hyperray) -->
          <tr>
            <td class="proj-teaser">
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Min_Geometric_Viewpoint_Learning_with_Hyper-Rays_and_Harmonics_Encoding_ICCV_2023_paper.pdf"><img src="./hyperray-teaser.png"></a>
            </td>
            <td class="proj-des">
              <b>
                <a class="title" href="https://openaccess.thecvf.com/content/ICCV2023/papers/Min_Geometric_Viewpoint_Learning_with_Hyper-Rays_and_Harmonics_Encoding_ICCV_2023_paper.pdf">
                  Geometric Viewpoint Learning with Hyper-Rays and Harmonics Encoding
                </a>
              </b><br>
              <b>Zhixiang Min</b>, 
              <a class="author" href="https://jdibenes.github.io/">Juan Carlos Dibene</a>,
              <a class="author" href="https://enriquedunn.github.io/">Enrique Dunn</a> <br>
              <b>ICCV 2023 </b> <br>
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Min_Geometric_Viewpoint_Learning_with_Hyper-Rays_and_Harmonics_Encoding_ICCV_2023_paper.pdf">[thecvf]</a>
              <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Min_Geometric_Viewpoint_Learning_ICCV_2023_supplemental.pdf">[appendix]</a>
              <br>
              </b>
            </td>

            <!-- iccv 2023 (GPM) -->
            <tr>
              <td class="proj-teaser">
                <a href="https://jdibenes.github.io/"><img src="./gpm-teaser.png"></a>
              </td>
              <td class="proj-des">
                <b>
                  <a class="title" href="https://jdibenes.github.io/">
                    General Planar Motion from a Pair of 3D Correspondences
                  </a>
                </b><br>
                <a class="author" href="https://jdibenes.github.io/">Juan Carlos Dibene</a>,
                <b>Zhixiang Min</b>, 
                <a class="author" href="https://enriquedunn.github.io/">Enrique Dunn</a> <br>
                <b>ICCV 2023 <font color="RED">(Oral Presentation)</font></b> <br>
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Dibene_General_Planar_Motion_from_a_Pair_of_3D_Correspondences_ICCV_2023_paper.pdf">[thecvf]</a>
                <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Dibene_General_Planar_Motion_ICCV_2023_supplemental.pdf">[appendix]</a>
                <a href="https://app.imagina.com/iccv-2023/370716">[video(6:30)]</a>
                
                <br>
                </b>
              </td>

          <!-- cvpr 2023 -->
          <tr>
            <td class="proj-teaser">
              <a href="https://arxiv.org/abs/2305.17763"><img src="./neurocs-teaser.gif"></a>
            </td>
            <td class="proj-des">
              <b>
                <a class="title" href="https://arxiv.org/abs/2305.17763">
                NeurOCS: Neural NOCS Supervision for Monocular 3D Object Localization
                </a>
              </b><br>
              <b>Zhixiang Min</b>, 
              <a class="author" href="https://bbzh.github.io/">Bingbing Zhuang</a>,
              <a class="author" href="https://samschulter.github.io/">Samuel Schulter</a>, 
              <a class="author" href="https://sites.google.com/site/buyuliu911/home">Buyu Liu</a>, 
              <a class="author" href="https://enriquedunn.github.io/">Enrique Dunn</a>, 
              <a class="author" href="https://cseweb.ucsd.edu//~mkchandraker/">Manmohan Chandraker</a> <br>
              <b>CVPR 2023 </b> <br>
              <a href="https://arxiv.org/abs/2305.17763">[arxiv]</a>
              <a href="./neurocs-appendix.pdf">[appendix]</a>
              <a href="https://www.youtube.com/watch?v=TrE4NGyc4SQ">[video]</a>
              <br>
              </b>
            </td>

          <!-- cvpr 2022 -->
          <tr>
            <td class="proj-teaser">
              <a href="https://github.com/zillow/laser"><img src="./laser-teaser.gif"></a>
              </td>
            <td class="proj-des">
              <b>
                <a class="title" href="https://github.com/zillow/laser">
                LASER: LAtent SpacE Rendering for 2D Visual Localization
                </a>
              </b><br>
              <b>Zhixiang Min</b>, 
              <a class="author" href="https://www.najikhosravan.com/">Naji Khosravan</a>,
              <a class="author" href="https://zachbessinger.com/">Zachary Bessinger</a>, 
              <a class="author" href="https://scholar.google.com/citations?user=9D2jiacAAAAJ">Manjunath Narayana</a>, 
              <a class="author" href="http://www.singbingkang.com/">Sing Bing Kang</a>, 
              <a class="author" href="https://enriquedunn.github.io/">Enrique Dunn</a>, 
              <a class="author" href="https://iboyadzhiev.github.io/">Ivaylo Boyadzhiev</a> <br>
              <b>CVPR 2022 <font color="RED">(Oral Presentation)</font></b> <br>
              <a href="https://arxiv.org/abs/2204.00157">[arxiv]</a>
              <a href="./laser-appendix.pdf">[appendix]</a>
              <a href="https://www.youtube.com/watch?v=yb7bj2VZkSg">[video]</a>
              <a href="https://github.com/zillow/laser">[code]</a>
              <br>
              </b>
            </td>

            <!-- icra 2021 -->
            <tr>
              <td class="proj-teaser">
                <a href="https://github.com/htkseason/VOLDOR"><img src="./voldorslam-teaser.gif"></a>
              </td>
              <td class="proj-des">
                <b>
                  <a class="title" href="https://github.com/htkseason/VOLDOR">
                  VOLDOR-SLAM: For the times when feature-based or direct methods are not good enough 
                </a>
                </b><br>
                  <b>Zhixiang Min</b>, 
                  <a class="author" href="https://enriquedunn.github.io/">Enrique Dunn</a>  
                  <br> <b>ICRA 2021</b> <br>
                <a href="https://arxiv.org/abs/2104.06800">[arxiv]</a>
                <a href="https://www.youtube.com/watch?v=sz8O953cXc0">[video]</a>
                <a href="https://github.com/htkseason/VOLDOR">[code]</a> <br>
                <b><span class="proj-des-highlight">Our SLAM solution has achieved 2nd place in both
                    tracks (monocular, stereo) of 
                    <a href="https://sites.google.com/view/vislocslamcvpr2020/slam-challenge">CVPR'20 SLAM Challenge</a> 
                    while retaining realtime.</span>
                </b>
              </td>
            </tr>

            <!-- cvpr 2020 voldor -->
            <tr>
              <td class="proj-teaser">
              <a href="https://github.com/htkseason/VOLDOR"><img src="./voldor-teaser.gif"></a>
              </td>
              <td class="proj-des">
                <b>
                <a class="title" href="https://github.com/htkseason/VOLDOR">
                VOLDOR: Visual Odometry from Log-logistic Dense Optical flow Residuals
                </a>
              </b><br>
                  <b>Zhixiang Min</b>, 
                  <a class="author" href="https://ihollywhy.github.io/">Yiding Yang</a>, 
                  <a class="author" href="https://enriquedunn.github.io/">Enrique Dunn</a> <br> 
                <b>CVPR 2020 <font color="RED">(Oral Presentation)</font></b> <br>
                <a href="https://arxiv.org/abs/2104.06789">[arxiv]</a>
                <a href="./voldor-appendix.pdf">[appendix]</a>
                <a href="https://www.youtube.com/watch?v=wlWjSTiyE4s">[video]</a>
                <a href="https://github.com/htkseason/VOLDOR">[code]</a> <br> 
                <!--- <b><p class="proj-detail">
                  A visual odometry framework using dense optical flows (OF) as input. 
                  We address the fine-grained flow residual modelling and design an efficient optimization method for dense OFs. 
                  The method achieves superior performance
                </p>
                --->
              </td>
            </tr>

          </tbody>
        </table>
      </div>
    </div>
  </div>

</body>

</html>